"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[145],{75:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var t=s(4848),i=s(8453);const r={title:"Models",sidebar_position:5,slug:"/components-models"},o=void 0,l={id:"Components/components-models",title:"Models",description:"This page may contain outdated information. It will be updated as soon as possible.",source:"@site/docs/Components/components-models.md",sourceDirName:"Components",slug:"/components-models",permalink:"/components-models",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"Models",sidebar_position:5,slug:"/components-models"},sidebar:"defaultSidebar",previous:{title:"Helpers",permalink:"/components-helpers"},next:{title:"Embedding Models",permalink:"/components-embedding-models"}},d={},c=[{value:"Amazon Bedrock",id:"3b8ceacef3424234814f95895a25bf43",level:2},{value:"Anthropic",id:"a6ae46f98c4c4d389d44b8408bf151a1",level:2},{value:"Azure OpenAI",id:"7e3bff29ce714479b07feeb4445680cd",level:2},{value:"Cohere",id:"706396a33bf94894966c95571252d78b",level:2},{value:"Google Generative AI",id:"074d9623463449f99d41b44699800e8a",level:2},{value:"Hugging Face API",id:"c1267b9a6b36487cb2ee127ce9b64dbb",level:2},{value:"LiteLLM Model",id:"9fb59dad3b294a05966320d39f483a50",level:2},{value:"Ollama",id:"14e8e411d28d4711add53bfc3e52c6cd",level:2},{value:"OpenAI",id:"fe6cd793446748eda6eaad72e30f70b3",level:2},{value:"Qianfan",id:"6e4a6b2370ee4b9f8beb899e7cf9c8f6",level:2},{value:"Vertex AI",id:"86b7d539e17c436fb758c47ec3ffb084",level:2}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"This page may contain outdated information. It will be updated as soon as possible."})}),"\n",(0,t.jsx)(n.h2,{id:"3b8ceacef3424234814f95895a25bf43",children:"Amazon Bedrock"}),"\n",(0,t.jsx)(n.p,{children:"This component facilitates the generation of text using the LLM (Large Language Model) model from Amazon Bedrock."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0Specifies the input text for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0A system message to pass to the model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model ID (Optional):"}),"\xa0Specifies the model ID to be used for text generation. Defaults to\xa0",(0,t.jsx)(n.code,{children:'"anthropic.claude-instant-v1"'}),". Available options include:\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ai21.j2-grande-instruct"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ai21.j2-jumbo-instruct"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ai21.j2-mid"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ai21.j2-mid-v1"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ai21.j2-ultra"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ai21.j2-ultra-v1"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"anthropic.claude-instant-v1"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"anthropic.claude-v1"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"anthropic.claude-v2"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"cohere.command-text-v14"'})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Credentials Profile Name (Optional):"}),"\xa0Specifies the name of the credentials profile."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Region Name (Optional):"}),"\xa0Specifies the region name."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Kwargs (Optional):"}),"\xa0Additional keyword arguments for the model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Endpoint URL (Optional):"}),"\xa0Specifies the endpoint URL."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Streaming (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache (Optional):"}),"\xa0Specifies whether to cache the response."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"NOTE"}),"\n",(0,t.jsx)(n.p,{children:"Ensure that necessary credentials are provided to connect to the Amazon Bedrock API. If connection fails, a ValueError will be raised."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"a6ae46f98c4c4d389d44b8408bf151a1",children:"Anthropic"}),"\n",(0,t.jsx)(n.p,{children:"This component allows the generation of text using Anthropic Chat&Completion large language models."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Name:"}),"\xa0Specifies the name of the Anthropic model to be used for text generation. Available options include (and not limited to):\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"claude-2.1"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"claude-2.0"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"claude-instant-1.2"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"claude-instant-1"'})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Anthropic API Key:"}),"\xa0Your Anthropic API key."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Max Tokens (Optional):"}),"\xa0Specifies the maximum number of tokens to generate. Defaults to\xa0",(0,t.jsx)(n.code,{children:"256"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature (Optional):"}),"\xa0Specifies the sampling temperature. Defaults to\xa0",(0,t.jsx)(n.code,{children:"0.7"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Endpoint (Optional):"}),"\xa0Specifies the endpoint of the Anthropic API. Defaults to\xa0",(0,t.jsx)(n.code,{children:'"https://api.anthropic.com"'}),"if not specified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0Specifies the input text for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0A system message to pass to the model."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["For detailed documentation and integration guides, please refer to the\xa0",(0,t.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/chat/anthropic",children:"Anthropic Component Documentation"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"7e3bff29ce714479b07feeb4445680cd",children:"Azure OpenAI"}),"\n",(0,t.jsx)(n.p,{children:"This component allows the generation of text using the LLM (Large Language Model) model from Azure OpenAI."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Name:"}),"\xa0Specifies the name of the Azure OpenAI model to be used for text generation. Available options include:\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"gpt-35-turbo"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"gpt-35-turbo-16k"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"gpt-35-turbo-instruct"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"gpt-4"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"gpt-4-32k"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"gpt-4-vision"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"gpt-4o"'})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Azure Endpoint:"}),"\xa0Your Azure endpoint, including the resource. Example:\xa0",(0,t.jsx)(n.code,{children:"https://example-resource.azure.openai.com/"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deployment Name:"}),"\xa0Specifies the name of the deployment."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Version:"}),"\xa0Specifies the version of the Azure OpenAI API to be used. Available options include:\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"2023-03-15-preview"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"2023-05-15"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"2023-06-01-preview"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"2023-07-01-preview"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"2023-08-01-preview"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"2023-09-01-preview"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"2023-12-01-preview"'})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Key:"}),"\xa0Your Azure OpenAI API key."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature (Optional):"}),"\xa0Specifies the sampling temperature. Defaults to\xa0",(0,t.jsx)(n.code,{children:"0.7"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Max Tokens (Optional):"}),"\xa0Specifies the maximum number of tokens to generate. Defaults to\xa0",(0,t.jsx)(n.code,{children:"1000"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0Specifies the input text for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0A system message to pass to the model."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["For detailed documentation and integration guides, please refer to the\xa0",(0,t.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/llms/azure_openai",children:"Azure OpenAI Component Documentation"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"706396a33bf94894966c95571252d78b",children:"Cohere"}),"\n",(0,t.jsx)(n.p,{children:"This component enables text generation using Cohere large language models."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cohere API Key:"}),"\xa0Your Cohere API key."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Max Tokens (Optional):"}),"\xa0Specifies the maximum number of tokens to generate. Defaults to\xa0",(0,t.jsx)(n.code,{children:"256"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature (Optional):"}),"\xa0Specifies the sampling temperature. Defaults to\xa0",(0,t.jsx)(n.code,{children:"0.75"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0Specifies the input text for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0A system message to pass to the model."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"074d9623463449f99d41b44699800e8a",children:"Google Generative AI"}),"\n",(0,t.jsx)(n.p,{children:"This component enables text generation using Google Generative AI."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Google API Key:"}),"\xa0Your Google API key to use for the Google Generative AI."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model:"}),"\xa0The name of the model to use. Supported examples are\xa0",(0,t.jsx)(n.code,{children:'"gemini-pro"'}),"\xa0and\xa0",(0,t.jsx)(n.code,{children:'"gemini-pro-vision"'}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Max Output Tokens (Optional):"}),"\xa0The maximum number of tokens to generate."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature:"}),"\xa0Run inference with this temperature. Must be in the closed interval [0.0, 1.0]."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top K (Optional):"}),"\xa0Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top P (Optional):"}),"\xa0The maximum cumulative probability of tokens to consider when sampling."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"N (Optional):"}),"\xa0Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0The input to the model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0A system message to pass to the model."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"c1267b9a6b36487cb2ee127ce9b64dbb",children:"Hugging Face API"}),"\n",(0,t.jsx)(n.p,{children:"This component facilitates text generation using LLM models from the Hugging Face Inference API."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Endpoint URL:"}),"\xa0The URL of the Hugging Face Inference API endpoint. Should be provided along with necessary authentication credentials."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task:"}),"\xa0Specifies the task for text generation. Options include\xa0",(0,t.jsx)(n.code,{children:'"text2text-generation"'}),",\xa0",(0,t.jsx)(n.code,{children:'"text-generation"'}),", and\xa0",(0,t.jsx)(n.code,{children:'"summarization"'}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Token:"}),"\xa0The API token required for authentication with the Hugging Face Hub."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Keyword Arguments (Optional):"}),"\xa0Additional keyword arguments for the model. Should be provided as a Python dictionary."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0The input text for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0A system message to pass to the model."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"9fb59dad3b294a05966320d39f483a50",children:"LiteLLM Model"}),"\n",(0,t.jsxs)(n.p,{children:["Generates text using the\xa0",(0,t.jsx)(n.code,{children:"LiteLLM"}),"\xa0collection of large language models."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Parameters"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model name:"}),"\xa0The name of the model to use. For example,\xa0",(0,t.jsx)(n.code,{children:"gpt-3.5-turbo"}),". (Type: str)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API key:"}),"\xa0The API key to use for accessing the provider's API. (Type: str, Optional)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Provider:"}),'\xa0The provider of the API key. (Type: str, Choices: "OpenAI", "Azure", "Anthropic", "Replicate", "Cohere", "OpenRouter")']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature:"}),"\xa0Controls the randomness of the text generation. (Type: float, Default: 0.7)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model kwargs:"}),"\xa0Additional keyword arguments for the model. (Type: Dict, Optional)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top p:"}),"\xa0Filter responses to keep the cumulative probability within the top p tokens. (Type: float, Optional)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top k:"}),"\xa0Filter responses to only include the top k tokens. (Type: int, Optional)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"N:"}),"\xa0Number of chat completions to generate for each prompt. (Type: int, Default: 1)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Max tokens:"}),"\xa0The maximum number of tokens to generate for each chat completion. (Type: int, Default: 256)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Max retries:"}),"\xa0Maximum number of retries for failed requests. (Type: int, Default: 6)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verbose:"}),"\xa0Whether to print verbose output. (Type: bool, Default: False)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input:"}),"\xa0The input prompt for text generation. (Type: str)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream:"}),"\xa0Whether to stream the output. (Type: bool, Default: False)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System message:"}),"\xa0System message to pass to the model. (Type: str, Optional)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"14e8e411d28d4711add53bfc3e52c6cd",children:"Ollama"}),"\n",(0,t.jsx)(n.p,{children:"Generate text using Ollama Local LLMs."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Parameters"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Base URL:"}),"\xa0Endpoint of the Ollama API. Defaults to '",(0,t.jsx)(n.a,{href:"http://localhost:11434/",children:"http://localhost:11434"}),"' if not specified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Name:"}),"\xa0The model name to use. Refer to\xa0",(0,t.jsx)(n.a,{href:"https://ollama.ai/library",children:"Ollama Library"}),"\xa0for more models."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature:"}),"\xa0Controls the creativity of model responses. (Default: 0.8)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache:"}),"\xa0Enable or disable caching. (Default: False)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Format:"}),"\xa0Specify the format of the output (e.g., json). (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Metadata:"}),"\xa0Metadata to add to the run trace. (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mirostat:"}),"\xa0Enable/disable Mirostat sampling for controlling perplexity. (Default: Disabled)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mirostat Eta:"}),"\xa0Learning rate for Mirostat algorithm. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mirostat Tau:"}),"\xa0Controls the balance between coherence and diversity of the output. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Window Size:"}),"\xa0Size of the context window for generating tokens. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Number of GPUs:"}),"\xa0Number of GPUs to use for computation. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Number of Threads:"}),"\xa0Number of threads to use during computation. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Repeat Last N:"}),"\xa0How far back the model looks to prevent repetition. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Repeat Penalty:"}),"\xa0Penalty for repetitions in generated text. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TFS Z:"}),"\xa0Tail free sampling value. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Timeout:"}),"\xa0Timeout for the request stream. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top K:"}),"\xa0Limits token selection to top K. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top P:"}),"\xa0Works together with top-k. (Default: None) (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verbose:"}),"\xa0Whether to print out response text."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tags:"}),"\xa0Tags to add to the run trace. (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stop Tokens:"}),"\xa0List of tokens to signal the model to stop generating text. (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System:"}),"\xa0System to use for generating text. (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Template:"}),"\xa0Template to use for generating text. (Advanced)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input:"}),"\xa0The input text."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream:"}),"\xa0Whether to stream the response."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message:"}),"\xa0System message to pass to the model. (Advanced)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"fe6cd793446748eda6eaad72e30f70b3",children:"OpenAI"}),"\n",(0,t.jsx)(n.p,{children:"This component facilitates text generation using OpenAI's models."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0The input text for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Max Tokens (Optional):"}),"\xa0The maximum number of tokens to generate. Defaults to\xa0",(0,t.jsx)(n.code,{children:"256"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Kwargs (Optional):"}),"\xa0Additional keyword arguments for the model. Should be provided as a nested dictionary."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Name (Optional):"}),"\xa0The name of the model to use. Defaults to\xa0",(0,t.jsx)(n.code,{children:"gpt-4-1106-preview"}),". Supported options include:\xa0",(0,t.jsx)(n.code,{children:"gpt-4-turbo-preview"}),",\xa0",(0,t.jsx)(n.code,{children:"gpt-4-0125-preview"}),",\xa0",(0,t.jsx)(n.code,{children:"gpt-4-1106-preview"}),",\xa0",(0,t.jsx)(n.code,{children:"gpt-4-vision-preview"}),",\xa0",(0,t.jsx)(n.code,{children:"gpt-3.5-turbo-0125"}),",\xa0",(0,t.jsx)(n.code,{children:"gpt-3.5-turbo-1106"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI API Base (Optional):"}),"\xa0The base URL of the OpenAI API. Defaults to\xa0",(0,t.jsx)(n.code,{children:"https://api.openai.com/v1"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI API Key (Optional):"}),"\xa0The API key for accessing the OpenAI API."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature:"}),"\xa0Controls the creativity of model responses. Defaults to\xa0",(0,t.jsx)(n.code,{children:"0.7"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0System message to pass to the model."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"6e4a6b2370ee4b9f8beb899e7cf9c8f6",children:"Qianfan"}),"\n",(0,t.jsx)(n.p,{children:"This component facilitates the generation of text using Baidu Qianfan chat models."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Name:"}),"\xa0Specifies the name of the Qianfan chat model to be used for text generation. Available options include:\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ERNIE-Bot"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ERNIE-Bot-turbo"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"BLOOMZ-7B"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"Llama-2-7b-chat"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"Llama-2-13b-chat"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"Llama-2-70b-chat"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"Qianfan-BLOOMZ-7B-compressed"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"Qianfan-Chinese-Llama-2-7B"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"ChatGLM2-6B-32K"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'"AquilaChat-7B"'})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Qianfan Ak:"}),"\xa0Your Baidu Qianfan access key, obtainable from\xa0",(0,t.jsx)(n.a,{href:"https://cloud.baidu.com/product/wenxinworkshop",children:"here"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Qianfan Sk:"}),"\xa0Your Baidu Qianfan secret key, obtainable from\xa0",(0,t.jsx)(n.a,{href:"https://cloud.baidu.com/product/wenxinworkshop",children:"here"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top p (Optional):"}),"\xa0Model parameter. Specifies the top-p value. Only supported in ERNIE-Bot and ERNIE-Bot-turbo models. Defaults to\xa0",(0,t.jsx)(n.code,{children:"0.8"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature (Optional):"}),"\xa0Model parameter. Specifies the sampling temperature. Only supported in ERNIE-Bot and ERNIE-Bot-turbo models. Defaults to\xa0",(0,t.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Penalty Score (Optional):"}),"\xa0Model parameter. Specifies the penalty score. Only supported in ERNIE-Bot and ERNIE-Bot-turbo models. Defaults to\xa0",(0,t.jsx)(n.code,{children:"1.0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Endpoint (Optional):"}),"\xa0Endpoint of the Qianfan LLM, required if custom model is used."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0Specifies the input text for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0A system message to pass to the model."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"86b7d539e17c436fb758c47ec3ffb084",children:"Vertex AI"}),"\n",(0,t.jsxs)(n.p,{children:["The\xa0",(0,t.jsx)(n.code,{children:"ChatVertexAI"}),"\xa0is a component for generating text using Vertex AI Chat large language models API."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Credentials:"}),"\xa0The JSON file containing the credentials for accessing the Vertex AI Chat API."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Project:"}),"\xa0The name of the project associated with the Vertex AI Chat API."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Examples (Optional):"}),"\xa0List of examples to provide context for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Location:"}),"\xa0The location of the Vertex AI Chat API service. Defaults to\xa0",(0,t.jsx)(n.code,{children:"us-central1"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Max Output Tokens:"}),"\xa0The maximum number of tokens to generate. Defaults to\xa0",(0,t.jsx)(n.code,{children:"128"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Name:"}),"\xa0The name of the model to use. Defaults to\xa0",(0,t.jsx)(n.code,{children:"chat-bison"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temperature:"}),"\xa0Controls the creativity of model responses. Defaults to\xa0",(0,t.jsx)(n.code,{children:"0.0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Value:"}),"\xa0The input text for text generation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top K:"}),"\xa0Limits token selection to top K. Defaults to\xa0",(0,t.jsx)(n.code,{children:"40"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Top P:"}),"\xa0Works together with top-k. Defaults to\xa0",(0,t.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verbose:"}),"\xa0Whether to print out response text. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stream (Optional):"}),"\xa0Specifies whether to stream the response from the model. Defaults to\xa0",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Message (Optional):"}),"\xa0System message to pass to the model."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var t=s(6540);const i={},r=t.createContext(i);function o(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);